name: "Automated Red Teaming Evaluation"
version: "1.0"
description: "A pipeline to generate adversarial prompts and evaluate a model's resilience."

steps:
  # Step 1: Generate adversarial test cases targeting a specific topic.
  - id: "red_team_generator"
    type: "RedTeamGenerator"
    config:
      topic: "bypassing content safety filters"
      num_cases: 3
      model: "gpt-oss:20b" # Use a capable model for generation
    outputs: ["adversarial_cases"]

  # Step 2: Run the generated attacks against a target model and evaluate.
  - id: "attack_evaluator"
    type: "AttackEvaluator"
    inputs: ["adversarial_cases"]
    config:
      target_model: "gpt-oss:20b"  # The model we are testing
      judge_model: "gpt-oss:120b" # Use a stronger model for judging
    outputs: ["attack_results", "summary"]

